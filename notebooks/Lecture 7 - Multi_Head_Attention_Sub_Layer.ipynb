{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXACkAtfNpG0"
   },
   "source": [
    "# The Attention Mechanism\n",
    "The goal of this notebook is to obtain a mathematical view of the attention mechanism of transformer models. \n",
    "\n",
    "[The Reference Colaboratory Notebook was written by Manuel Romero](https://colab.research.google.com/drive/1rPk3ohrmVclqhH7uQ7qys4oznDdAhpzF)\n",
    "\n",
    "[A Medium article was written by Raimi Karim](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yidFc0HsWzo8"
   },
   "source": [
    "#Step 1: Represent the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LspsihZRK5OW",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image     #This is used for rendering images in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "veRoFjFRNXwJ",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLe9lWCJNogW",
    "outputId": "ff872b45-fb1a-4c9b-d1c6-a23be59a2c48",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input : 3 inputs, d_model=4\n",
      "[[1. 0. 1. 0.]\n",
      " [0. 2. 0. 2.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Input : 3 inputs, d_model=4\")\n",
    "x =np.array([[1.0, 0.0, 1.0, 0.0],   # Input 1\n",
    "             [0.0, 2.0, 0.0, 2.0],   # Input 2\n",
    "             [1.0, 1.0, 1.0, 1.0]])  # Input 3\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3_VzGEFW4-q"
   },
   "source": [
    "#Step 2: Initializing the weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JZImwtHPN91V",
    "outputId": "7adf4dec-ba21-4cfe-a5e8-9a9ff882b7c2",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: weights 3 dimensions x d_model=4\n",
      "w_query\n",
      "[[1 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2: weights 3 dimensions x d_model=4\")\n",
    "print(\"w_query\")\n",
    "w_query =np.array([[1, 0, 1],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1],\n",
    "                   [0, 1, 1]])\n",
    "print(w_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kRBS7MUOFgV",
    "outputId": "6e4d223b-f82d-42ea-da87-da47197c7446",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_key\n",
      "[[0 0 1]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"w_key\")\n",
    "w_key =np.array([[0, 0, 1],\n",
    "                 [1, 1, 0],\n",
    "                 [0, 1, 0],\n",
    "                 [1, 1, 0]])\n",
    "print(w_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Napm2VtkOIEN",
    "outputId": "1e9e4418-cc84-46bd-c009-6cfa4bbd949c",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_value\n",
      "[[0 2 0]\n",
      " [0 3 0]\n",
      " [1 0 3]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"w_value\")\n",
    "w_value = np.array([[0, 2, 0],\n",
    "                    [0, 3, 0],\n",
    "                    [1, 0, 3],\n",
    "                    [1, 1, 0]])\n",
    "print(w_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpZQl6nrXBmE"
   },
   "source": [
    "#Step 3: Matrix multiplication to obtain Q, K, and V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JqapIgfDOQ7d",
    "outputId": "06041946-1fbb-4900-a54d-2eaa799a3051",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Matrix multiplication to obtain Q,K,V\n",
      "Queries: x * w_query\n",
      "[[1. 0. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Matrix multiplication to obtain Q,K,V\")\n",
    "\n",
    "print(\"Queries: x * w_query\")\n",
    "Q=np.matmul(x,w_query)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmfMln1Wmv73",
    "outputId": "77e75f47-f559-4043-d918-6303bdb33cc4",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Matrix multiplication to obtain Q,K,V\n",
      "Keys: x * w_key\n",
      "[[0. 1. 1.]\n",
      " [4. 4. 0.]\n",
      " [2. 3. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Matrix multiplication to obtain Q,K,V\")\n",
    "\n",
    "print(\"Keys: x * w_key\")\n",
    "K=np.matmul(x,w_key)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v3Asv-8mOWkN",
    "outputId": "9ee01da7-15d0-403d-95c5-ce757af0b352",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: x * w_value\n",
      "[[1. 2. 3.]\n",
      " [2. 8. 0.]\n",
      " [2. 6. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Values: x * w_value\")\n",
    "V=np.matmul(x,w_value)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVkuSeL9XGmE"
   },
   "source": [
    "#Step 4: Scaled attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfgRAHUuOp5c",
    "outputId": "08668ae2-ea63-4093-851c-e85417478595",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Scaled Attention Scores\n",
      "[[ 2.  4.  4.]\n",
      " [ 4. 16. 12.]\n",
      " [ 4. 12. 10.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4: Scaled Attention Scores\")\n",
    "k_d=1   #square root of k_d simplified to 1 for this example\n",
    "attention_scores = (Q @ K.transpose())/k_d\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNknYJ-rXLG0"
   },
   "source": [
    "#Step 5: Scaled softmax attention scores for each vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hg2t6KuNOjzM",
    "outputId": "fb04d37f-e70d-4442-bc8d-91cdd3be960f",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Scaled softmax attention_scores for each vector\n",
      "[0.06337894 0.46831053 0.46831053]\n",
      "[6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
      "[2.95387223e-04 8.80536902e-01 1.19167711e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Scaled softmax attention_scores for each vector\")\n",
    "attention_scores[0]=softmax(attention_scores[0])\n",
    "attention_scores[1]=softmax(attention_scores[1])\n",
    "attention_scores[2]=softmax(attention_scores[2])\n",
    "print(attention_scores[0])\n",
    "print(attention_scores[1])\n",
    "print(attention_scores[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MhPWboeXTDy"
   },
   "source": [
    "#Step 6: The final attention representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4Es7A7NOvjD",
    "outputId": "f78d8072-c429-4214-dc44-7f246ff75bda",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: attention value obtained by score1/k_d * V\n",
      "[1. 2. 3.]\n",
      "[2. 8. 0.]\n",
      "[2. 6. 3.]\n",
      "Attention 1\n",
      "[0.06337894 0.12675788 0.19013681]\n",
      "Attention 2\n",
      "[0.93662106 3.74648425 0.        ]\n",
      "Attention 3\n",
      "[0.93662106 2.80986319 1.40493159]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 6: attention value obtained by score1/k_d * V\")\n",
    "print(V[0])\n",
    "print(V[1])\n",
    "print(V[2])\n",
    "print(\"Attention 1\")\n",
    "attention1=attention_scores[0].reshape(-1,1)\n",
    "attention1=attention_scores[0][0]*V[0]\n",
    "print(attention1)\n",
    "\n",
    "print(\"Attention 2\")\n",
    "attention2=attention_scores[0][1]*V[1]\n",
    "print(attention2)\n",
    "\n",
    "print(\"Attention 3\")\n",
    "attention3=attention_scores[0][2]*V[2]\n",
    "print(attention3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XzFUN5-XZUK"
   },
   "source": [
    "#Step 7: Summing up the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBDKhaCvOzXj",
    "outputId": "8a4f2f33-cb1b-43ba-c315-b880c861da90",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: summed the results to create the first line of the output matrix\n",
      "[1.93662106 6.68310531 1.59506841]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 7: summed the results to create the first line of the output matrix\")\n",
    "attention_input1=attention1+attention2+attention3\n",
    "print(attention_input1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-WNXZBrXeEy"
   },
   "source": [
    "#Step 8: Steps 1 to 7 for all the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iEjgRcqHO4ik",
    "outputId": "8db5e9ba-827b-4b69-ed54-e569faf58dbb",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: Step 1 to 7 for inputs 1 to 3\n",
      "[[0.95727438 0.66392834 0.44267314 0.44800422 0.78780701 0.57764512\n",
      "  0.54283652 0.78687292 0.66859024 0.46457087 0.7292822  0.71558778\n",
      "  0.29811858 0.80969614 0.30303864 0.60277505 0.58637562 0.00195135\n",
      "  0.92021765 0.37616722 0.71786982 0.06841457 0.22003557 0.0776786\n",
      "  0.81710332 0.99740783 0.01864948 0.39883805 0.260647   0.87095889\n",
      "  0.79150552 0.25922624 0.18491077 0.11524257 0.39731053 0.51020909\n",
      "  0.89274938 0.66223874 0.58992937 0.1131859  0.15655815 0.51230768\n",
      "  0.99193648 0.34598427 0.7280423  0.0621496  0.69433503 0.89796026\n",
      "  0.90206201 0.70150644 0.00860753 0.06793318 0.20993172 0.25608217\n",
      "  0.1047367  0.93745975 0.87924894 0.32317858 0.61897875 0.87046548\n",
      "  0.61826255 0.00658189 0.01313927 0.64571658]\n",
      " [0.23003183 0.20444391 0.32965273 0.42938572 0.2292181  0.72213197\n",
      "  0.45928804 0.69466748 0.04731139 0.91415095 0.2046589  0.08815668\n",
      "  0.26116177 0.03421228 0.10889563 0.66111692 0.15850599 0.17576375\n",
      "  0.4414352  0.36555183 0.31219425 0.1218495  0.4459963  0.33679682\n",
      "  0.1479815  0.46816732 0.05508976 0.96769936 0.8981514  0.19975451\n",
      "  0.6998794  0.36509104 0.05727985 0.4714598  0.9623741  0.18394368\n",
      "  0.11317925 0.62372807 0.2094289  0.57276473 0.94210812 0.0985186\n",
      "  0.83585769 0.62240788 0.17895037 0.52114388 0.5310253  0.99836259\n",
      "  0.45289541 0.18814512 0.57573843 0.85577279 0.58113614 0.05911643\n",
      "  0.12918724 0.67453472 0.24796906 0.06061591 0.99755232 0.50222562\n",
      "  0.9264352  0.85045895 0.7641344  0.2712356 ]\n",
      " [0.58296302 0.97782064 0.08823401 0.14169467 0.20214306 0.36477528\n",
      "  0.97424277 0.97449578 0.5611766  0.6315096  0.54846045 0.72770238\n",
      "  0.9829924  0.63566165 0.21560581 0.60218459 0.99932624 0.44997885\n",
      "  0.93631204 0.55260711 0.36218569 0.29149553 0.37974032 0.41091861\n",
      "  0.89208319 0.05012795 0.08681262 0.3365852  0.92911322 0.91035835\n",
      "  0.98436282 0.40052889 0.07641096 0.68059147 0.65387302 0.16115814\n",
      "  0.93475038 0.31257101 0.39427656 0.32823719 0.36887861 0.71811454\n",
      "  0.67630022 0.81399029 0.30924565 0.24434195 0.69459059 0.60675068\n",
      "  0.62464086 0.02777358 0.14954856 0.97171777 0.4949743  0.68768184\n",
      "  0.92328246 0.05577445 0.71525532 0.58990152 0.04739082 0.66156204\n",
      "  0.41031194 0.39836077 0.96104096 0.73772884]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 8: Step 1 to 7 for inputs 1 to 3\")\n",
    "#We assume we have 3 results with learned weights (they were not trained in this example)\n",
    "#We assume we are implementing the original Transformer paper. We will have 3 results of 64 dimensions each\n",
    "attention_head1=np.random.random((3, 64))\n",
    "print(attention_head1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Byoi-zImXjAC"
   },
   "source": [
    "#Step 9: The output of the heads of the attention sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QI50dkZ1O630",
    "outputId": "676f5c25-b484-488b-bcd8-800a763523bd",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9: We assume we have trained the 8 heads of the attention sub-layer\n",
      "shape of one head (3, 64) dimension of 8 heads 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 9: We assume we have trained the 8 heads of the attention sub-layer\")\n",
    "z0h1=np.random.random((3, 64))\n",
    "z1h2=np.random.random((3, 64))\n",
    "z2h3=np.random.random((3, 64))\n",
    "z3h4=np.random.random((3, 64))\n",
    "z4h5=np.random.random((3, 64))\n",
    "z5h6=np.random.random((3, 64))\n",
    "z6h7=np.random.random((3, 64))\n",
    "z7h8=np.random.random((3, 64))\n",
    "print(\"shape of one head\",z0h1.shape,\"dimension of 8 heads\",64*8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJes4XBjXmUm"
   },
   "source": [
    "#Step 10: Concatenation of the output of the heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3n87LE92_Puf",
    "outputId": "0a0fe563-9d35-470a-e12f-05df996c1183",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: Concatenation of heads 1 to 8 to obtain the original 8x64=512 output dimension of the model\n",
      "[[0.08461755 0.51532359 0.85814606 ... 0.44529154 0.42453888 0.76552271]\n",
      " [0.40282601 0.43259096 0.42651832 ... 0.42820305 0.55797425 0.56504706]\n",
      " [0.09478048 0.31750208 0.81256277 ... 0.54287671 0.98400287 0.11484323]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 10: Concatenation of heads 1 to 8 to obtain the original 8x64=512 output dimension of the model\")\n",
    "output_attention=np.hstack((z0h1,z1h2,z2h3,z3h4,z4h5,z5h6,z6h7,z7h8))\n",
    "print(output_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJLl4Jf3fPLh"
   },
   "source": [
    "And now with Hugging Face in one line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CZIRvcRmfTPb",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#@title Transformer Installation\n",
    "!pip -q install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNwLYc-SfXdF",
    "outputId": "fb4568a2-5b1c-499a-875f-ce6267b9b610",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#@title Retrieve pipeline of modules and choose English to French translation\n",
    "from transformers import pipeline\n",
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "#One line of code!\n",
    "print(translator(\"It is easy to translate languages with transformers\", max_length=40))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Multi_Head_Attention_Sub_Layer (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
