{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fjcTlyE3WvR"
   },
   "source": [
    "The goal of this notebook is to understand positional encoding and cosine similarity. Cosine similarity remains a solid NLP approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKJ8Saf6vR9b",
    "outputId": "0584cb90-d2dc-4fa8-c61d-aca0a1858528",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ramyaakula/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install gensim # Version Gensim 4.0.0 and above\n",
    "#!pip install torch\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7o7EeDUUu0Sh",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "\n",
    "dprint=0 # prints outputs if set to 1, default=0\n",
    "\n",
    "#‘text.txt’ file\n",
    "sample = open(\"text.txt\", \"r\")\n",
    "s = sample.read()\n",
    "\n",
    "# processing escape characters\n",
    "f = s.replace(\"\\n\", \" \")\n",
    "\n",
    "data = []\n",
    "\n",
    "# sentence parsing\n",
    "for i in sent_tokenize(f):\n",
    "\ttemp = []\n",
    "\t# tokenize the sentence into words\n",
    "\tfor j in word_tokenize(i):\n",
    "\t\ttemp.append(j.lower())\n",
    "\tdata.append(temp)\n",
    "\n",
    "# Creating Skip Gram model\n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 512, window = 5, sg = 1)\n",
    "\n",
    "# 1-The 2-black 3-cat 4-sat 5-on 6-the 7-couch 8-and 9-the 10-brown 11-dog 12-slept 13-on 14-the 15-rug.\n",
    "word1='black'\n",
    "word2='brown'\n",
    "pos1=2\n",
    "pos2=10\n",
    "a=model2.wv[word1]\n",
    "b=model2.wv[word2]\n",
    "\n",
    "if(dprint==1):\n",
    "        print(a)\n",
    "\n",
    "# compute cosine similarity\n",
    "dot = np.dot(a, b)\n",
    "norma = np.linalg.norm(a)\n",
    "normb = np.linalg.norm(b)\n",
    "cos = dot / (norma * normb)\n",
    "\n",
    "aa = a.reshape(1,512)\n",
    "ba = b.reshape(1,512)\n",
    "cos_lib = cosine_similarity(aa, ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EmBUq9MzxQxz",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pe1=aa.copy()\n",
    "pe2=aa.copy()\n",
    "pe3=aa.copy()\n",
    "paa=aa.copy()\n",
    "pba=ba.copy()\n",
    "d_model=512\n",
    "max_print=d_model\n",
    "max_length=20\n",
    "\n",
    "for i in range(0, max_print,2):\n",
    "                pe1[0][i] = math.sin(pos1 / (10000 ** ((2 * i)/d_model)))\n",
    "                paa[0][i] = (paa[0][i]*math.sqrt(d_model))+ pe1[0][i]\n",
    "                pe1[0][i+1] = math.cos(pos1 / (10000 ** ((2 * i)/d_model)))\n",
    "                paa[0][i+1] = (paa[0][i+1]*math.sqrt(d_model))+pe1[0][i+1]\n",
    "                if dprint==1:\n",
    "                        print(i,pe1[0][i],i+1,pe1[0][i+1])\n",
    "                        print(i,paa[0][i],i+1,paa[0][i+1])\n",
    "                        print(\"\\n\")\n",
    "\n",
    "#print(pe1)\n",
    "# A  method in Pytorch using torch.exp and math.log :\n",
    "max_len=max_length\n",
    "pe = torch.zeros(max_len, d_model)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#print(pe[:, 0::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgrXed2FwHDC",
    "outputId": "ec91e809-3a00-4d7f-e605-0c23cb36cad8",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black brown\n",
      "[[0.9995164]] word similarity\n",
      "[[0.8600013]] positional similarity\n",
      "[[0.9622497]] positional encoding similarity\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0, max_print,2):\n",
    "                pe2[0][i] = math.sin(pos2 / (10000 ** ((2 * i)/d_model)))\n",
    "                pba[0][i] = (pba[0][i]*math.sqrt(d_model))+ pe2[0][i]\n",
    "\n",
    "                pe2[0][i+1] = math.cos(pos2 / (10000 ** ((2 * i)/d_model)))\n",
    "                pba[0][i+1] = (pba[0][i+1]*math.sqrt(d_model))+ pe2[0][i+1]\n",
    "\n",
    "                if dprint==1:\n",
    "                        print(i,pe2[0][i],i+1,pe2[0][i+1])\n",
    "                        print(i,paa[0][i],i+1,paa[0][i+1])\n",
    "                        print(\"\\n\")\n",
    "\n",
    "print(word1,word2)\n",
    "cos_lib = cosine_similarity(aa, ba)\n",
    "print(cos_lib,\"word similarity\")\n",
    "cos_lib = cosine_similarity(pe1, pe2)\n",
    "print(cos_lib,\"positional similarity\")\n",
    "cos_lib = cosine_similarity(paa, pba)\n",
    "print(cos_lib,\"positional encoding similarity\")\n",
    "\n",
    "if dprint==1:\n",
    "        print(word1)\n",
    "        print(\"embedding\")\n",
    "        print(aa)\n",
    "        print(\"positional encoding\")\n",
    "        print(pe1)\n",
    "        print(\"encoded embedding\")\n",
    "        print(paa)\n",
    "\n",
    "        print(word2)\n",
    "        print(\"embedding\")\n",
    "        print(ba)\n",
    "        print(\"positional encoding\")\n",
    "        print(pe2)\n",
    "        print(\"encoded embedding\")\n",
    "        print(pba)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
